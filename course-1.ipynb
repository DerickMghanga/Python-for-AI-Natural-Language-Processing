{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ca4b7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5c03688f-7c9b-42bb-90fe-b182a4aa870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dc7a4e7c-13ba-4039-b156-1e484989d33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"imdb-dataset.csv\")\n",
    "df.head(20)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "945f7e3a-95ab-4a79-b5af-cf842a92dc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change all texts to lowercase\n",
    "df['review'] = df['review'].str.lower()\n",
    "df.head(20)\n",
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c4fc0111-4f63-446d-8c72-e92040acb464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all HTML tags\n",
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "92689ac3-d1f9-4597-a951-d89684226488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(remove_html_tags)\n",
    "#df.head(10)\n",
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ec2e09e3-521a-4367-a8c5-6eb911aa78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all URLs\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "60c10317-41f3-4c86-8aad-76a46d8e3905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(remove_url)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69433293-740a-4bef-83cb-1d617d35c177",
   "metadata": {},
   "source": [
    "<h1>Punctuation handling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bdcee724-2b33-40dc-aba2-bdfd2ed8e361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string, time\n",
    "\n",
    "exclude = string.punctuation\n",
    "print(exclude)\n",
    "\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('','',exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "db9c6c42-8b2b-4f90-b23a-cb626fd4babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df['review'][3])\n",
    "df['review'] = df['review'].apply(remove_punc)\n",
    "#df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3e7bc970-e402-4db6-b147-43c3e179d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social media chat words\n",
    "chat_words = {\n",
    "    'FYI':'For Your Information',\n",
    "    'ASAP': 'As Soon As Possible',\n",
    "    'BRB': 'Be Right Back',\n",
    "    'BTW':'By The Way',\n",
    "    'OMG':'Oh My God',\n",
    "    'IMO':'In My Opinion',\n",
    "    'LOL':'Laugh Out Loud',\n",
    "    'TTYL':'Talk To You Later',\n",
    "    'GTG':'Got To Go',\n",
    "    'TTYT':'Talk To You Tomorrow',\n",
    "    'IDK':\"I Don't Know\",\n",
    "    'TMI':'Too Much Information',\n",
    "    'IMHO':'In My Humble Opinion',\n",
    "    'ICYMI':'In Case You Missed It',\n",
    "    'AFAIK':'As Far As I Know',\n",
    "    'BTW':'By The Way',\n",
    "    'FAQ':'Frequently Asked Questions',\n",
    "    'TGIF':\"Thank God It's Friday\",\n",
    "    'FYA':'For Your Action',\n",
    "    'ICYMI':'In Case You Missed It',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "57ecba90-5a50-41ac-a69b-1a3276cf1e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ee48418e-3503-45b6-9d2e-bea5d6ec84a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do this work As Soon As Possible'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "chat_conversion('Do this work ASAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "75eaa31b-cef2-4ce6-8363-34df73c0d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(chat_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "26a07714-c9b8-456e-84be-4913a831572a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain conditions during several generations are modified in the same manner'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Incorrect text handling\n",
    "from textblob import TextBlob\n",
    "\n",
    "def correct_words(text):\n",
    "    textBlb = TextBlob(text)\n",
    "    return textBlb.correct().string\n",
    "\n",
    "# example\n",
    "incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the same manner'\n",
    "correct_words(incorrect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "67c34bf0-1d1b-42d4-a748-094d2847d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove grammartically incorrect words\n",
    "#df['review'] = df['review'].apply(correct_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7807467e-882e-43f3-b7fd-e47925f32ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bb4b6e93-dc98-4bb4-b8eb-24b2568ce751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably  all-time favourite movie,  story  selflessness, sacrifice  dedication'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)\n",
    "\n",
    "# example\n",
    "remove_stopwords(\"probably my all-time favourite movie, a story of selflessness, sacrifice and dedication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e2d040aa-487c-4e53-9f85-94146326bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "#df['review'] = df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090aeda-3ac6-47b6-a1e0-c0f12b2a594d",
   "metadata": {},
   "source": [
    "**Remove Emoji handle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a3b74976-1fec-463a-9cc5-dd7e79a78944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d552f26f-6b93-4837-8c65-7f2e06c1d788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love you'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji('I love youðŸ˜˜')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d5f33c1a-bae1-4d76-8db4-18ba6fad020c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LMAO. This is a joke'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji('LMAOðŸ˜€. This is a jokeðŸ˜’')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f928d-be73-4fae-901b-bb3284db00b1",
   "metadata": {},
   "source": [
    "<h1>Emoji Handling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b894a2b1-c7ba-4463-9273-2c5324ffe8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install emoji  #interpret emoji meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1998bd0b-21a0-4455-b79f-df6c0433cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import emoji\n",
    "# print(emoji.demojize('Python is ðŸ”¥'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc9165-c6b2-4aae-a603-eb99452e05ee",
   "metadata": {},
   "source": [
    "<h1>TOKENIZATION</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b099bb-e8a0-42df-85dd-f178e1e0d025",
   "metadata": {},
   "source": [
    "**1. Using split method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4ca227eb-c367-4aef-9ef6-28ca575f964c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'be', 'a', 'billionaire']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "sentence1 = \"I am going to be a billionaire\"\n",
    "sentence1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4872ea78-46e6-4a36-a9f1-56e713a214cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to be a billionaire',\n",
       " ' Change peoples lives',\n",
       " ' See you at the top']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sentence2 = \"I am going to be a billionaire. Change peoples lives. See you at the top\"\n",
    "sentence2.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee6015-5d39-4761-8d2d-eb5a7352c8a1",
   "metadata": {},
   "source": [
    "**2. Using Regular Expression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "45991d41-44ce-4721-8124-a053134a6810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13720\\756187364.py:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  tokens = re.findall(\"[\\w']+\", sent3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'the', 'moon']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent3 = 'I am going to the moon!'\n",
    "tokens = re.findall(\"[\\w']+\", sent3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3032711e-6420-4bf4-a3f1-c35166ff12c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentences = re.compile('[.!?] ').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a570b4dc-e820-4fa1-9a4b-9b10199cda68",
   "metadata": {},
   "source": [
    "**3. Using NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7b5b0d27-2ddc-4089-80a0-f96054832bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c2dc4f5c-f937-4f61-a65f-312ecac6769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'delhi', '!']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = 'I am going to visit delhi!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3ac47d02-37d5-47e5-b348-5ee951cefe41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5568b607-1876-4008-bc45-288267109aad",
   "metadata": {},
   "source": [
    "**4. Using Spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a8c03223-9ea6-4ce1-8dc7-6e6cdb4ef16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a71a60bd-ac6e-4847-a022-e1dc5b6087e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install blis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a65b739f-fedd-4baf-a9c4-1d0e1d609e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "#nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ca9a7357-8f0d-4c26-a11c-faff29f03fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc1 = nlp(sent5)\n",
    "# doc2 = nlp(sent6)\n",
    "# doc3 = nlp(sent7)\n",
    "# doc4 = nlp(sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc7b254-69d7-4789-85ff-5dd649df814b",
   "metadata": {},
   "source": [
    "<h1>Stemmer</h1>\n",
    "<p>text preprocessing technique used to reduce words to their root or base form.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2fcc515e-a6a8-4e5c-bd14-c1cd2bef8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8f862fef-57a2-4ca1-9841-7ca7a127f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "106fbd76-f516-4eed-8179-dfd73ca63449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a00ffa9c-a84a-43e9-a066-60b65a4bc767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get old despit my have seen it some 15 or more time in the last 25 year paul luka perform bring tear to my eye and bett davi in one of her veri few truli sympathet role is a delight the kid are as grandma say more like dressedup midget than children but that onli make them more fun to watch and the mother slow awaken to what happen in the world and under her own roof is believ and startl if i had a dozen thumb theyd all be up for thi movi'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie'\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d350605-2916-456d-86bc-0d4a0ecba488",
   "metadata": {},
   "source": [
    "<h1>Lemmatization</h1>\n",
    "<p>Lemmatization is the process of grouping together different inflected forms of the same word. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "12f59916-36de-43ae-b246-29213cd95db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "987d5617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405b428",
   "metadata": {},
   "source": [
    "NOTE: Stemming & lamatization are same to retrieve root words but lamatization is worked good. Lamatization is slow & stemming is fast"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
