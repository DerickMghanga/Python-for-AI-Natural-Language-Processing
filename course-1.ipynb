{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca4b7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c03688f-7c9b-42bb-90fe-b182a4aa870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc7a4e7c-13ba-4039-b156-1e484989d33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"imdb-dataset.csv\")\n",
    "df.head(20)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "945f7e3a-95ab-4a79-b5af-cf842a92dc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change all texts to lowercase\n",
    "df['review'] = df['review'].str.lower()\n",
    "df.head(20)\n",
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4fc0111-4f63-446d-8c72-e92040acb464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all HTML tags\n",
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92689ac3-d1f9-4597-a951-d89684226488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(remove_html_tags)\n",
    "#df.head(10)\n",
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec2e09e3-521a-4367-a8c5-6eb911aa78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all URLs\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60c10317-41f3-4c86-8aad-76a46d8e3905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(remove_url)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69433293-740a-4bef-83cb-1d617d35c177",
   "metadata": {},
   "source": [
    "<h1>Punctuation handling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdcee724-2b33-40dc-aba2-bdfd2ed8e361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string, time\n",
    "\n",
    "exclude = string.punctuation\n",
    "print(exclude)\n",
    "\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('','',exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db9c6c42-8b2b-4f90-b23a-cb626fd4babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df['review'][3])\n",
    "df['review'] = df['review'].apply(remove_punc)\n",
    "#df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e7bc970-e402-4db6-b147-43c3e179d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social media chat words\n",
    "chat_words = {\n",
    "    'FYI':'For Your Information',\n",
    "    'ASAP': 'As Soon As Possible',\n",
    "    'BRB': 'Be Right Back',\n",
    "    'BTW':'By The Way',\n",
    "    'OMG':'Oh My God',\n",
    "    'IMO':'In My Opinion',\n",
    "    'LOL':'Laugh Out Loud',\n",
    "    'TTYL':'Talk To You Later',\n",
    "    'GTG':'Got To Go',\n",
    "    'TTYT':'Talk To You Tomorrow',\n",
    "    'IDK':\"I Don't Know\",\n",
    "    'TMI':'Too Much Information',\n",
    "    'IMHO':'In My Humble Opinion',\n",
    "    'ICYMI':'In Case You Missed It',\n",
    "    'AFAIK':'As Far As I Know',\n",
    "    'BTW':'By The Way',\n",
    "    'FAQ':'Frequently Asked Questions',\n",
    "    'TGIF':\"Thank God It's Friday\",\n",
    "    'FYA':'For Your Action',\n",
    "    'ICYMI':'In Case You Missed It',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57ecba90-5a50-41ac-a69b-1a3276cf1e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee48418e-3503-45b6-9d2e-bea5d6ec84a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do this work As Soon As Possible'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "chat_conversion('Do this work ASAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75eaa31b-cef2-4ce6-8363-34df73c0d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(chat_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26a07714-c9b8-456e-84be-4913a831572a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain conditions during several generations are modified in the same manner'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Incorrect text handling\n",
    "from textblob import TextBlob\n",
    "\n",
    "def correct_words(text):\n",
    "    textBlb = TextBlob(text)\n",
    "    return textBlb.correct().string\n",
    "\n",
    "# example\n",
    "incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the same manner'\n",
    "correct_words(incorrect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67c34bf0-1d1b-42d4-a748-094d2847d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove grammartically incorrect words\n",
    "#df['review'] = df['review'].apply(correct_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7807467e-882e-43f3-b7fd-e47925f32ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb4b6e93-dc98-4bb4-b8eb-24b2568ce751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably  all-time favourite movie,  story  selflessness, sacrifice  dedication'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)\n",
    "\n",
    "# example\n",
    "remove_stopwords(\"probably my all-time favourite movie, a story of selflessness, sacrifice and dedication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2d040aa-487c-4e53-9f85-94146326bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "#df['review'] = df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090aeda-3ac6-47b6-a1e0-c0f12b2a594d",
   "metadata": {},
   "source": [
    "**Remove Emoji handle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3b74976-1fec-463a-9cc5-dd7e79a78944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d552f26f-6b93-4837-8c65-7f2e06c1d788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love you'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji('I love youðŸ˜˜')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5f33c1a-bae1-4d76-8db4-18ba6fad020c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LMAO. This is a joke'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji('LMAOðŸ˜€. This is a jokeðŸ˜’')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f928d-be73-4fae-901b-bb3284db00b1",
   "metadata": {},
   "source": [
    "<h1>Emoji Handling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b894a2b1-c7ba-4463-9273-2c5324ffe8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install emoji  #interpret emoji meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1998bd0b-21a0-4455-b79f-df6c0433cd98",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'emoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01memoji\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(emoji\u001b[38;5;241m.\u001b[39mdemojize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPython is ðŸ”¥\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'emoji'"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize('Python is ðŸ”¥'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc9165-c6b2-4aae-a603-eb99452e05ee",
   "metadata": {},
   "source": [
    "<h1>TOKENIZATION</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b099bb-e8a0-42df-85dd-f178e1e0d025",
   "metadata": {},
   "source": [
    "**1. Using split method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ca227eb-c367-4aef-9ef6-28ca575f964c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'be', 'a', 'billionaire']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "sentence1 = \"I am going to be a billionaire\"\n",
    "sentence1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4872ea78-46e6-4a36-a9f1-56e713a214cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to be a billionaire',\n",
       " ' Change peoples lives',\n",
       " ' See you at the top']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sentence2 = \"I am going to be a billionaire. Change peoples lives. See you at the top\"\n",
    "sentence2.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee6015-5d39-4761-8d2d-eb5a7352c8a1",
   "metadata": {},
   "source": [
    "**2. Using Regular Expression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45991d41-44ce-4721-8124-a053134a6810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sent3 = 'I am going to the moon!'\n",
    "tokens = re.findall(\"[\\w']+\", sent3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3032711e-6420-4bf4-a3f1-c35166ff12c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentences = re.compile('[.!?] ').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a570b4dc-e820-4fa1-9a4b-9b10199cda68",
   "metadata": {},
   "source": [
    "**3. Using NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b5b0d27-2ddc-4089-80a0-f96054832bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2dc4f5c-f937-4f61-a65f-312ecac6769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'delhi', '!']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = 'I am going to visit delhi!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ac47d02-37d5-47e5-b348-5ee951cefe41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5568b607-1876-4008-bc45-288267109aad",
   "metadata": {},
   "source": [
    "**4. Using Spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8c03223-9ea6-4ce1-8dc7-6e6cdb4ef16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.2-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.11-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.10-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Downloading thinc-8.3.2-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.13.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\desktop\\projects\\generative-ai-for-developers\\1.course\\venv\\lib\\site-packages (from spacy) (4.67.0)\n",
      "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.10.2-py3-none-any.whl.metadata (170 kB)\n",
      "Collecting jinja2 (from spacy)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting setuptools (from spacy)\n",
      "  Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\desktop\\projects\\generative-ai-for-developers\\1.course\\venv\\lib\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\admin\\desktop\\projects\\generative-ai-for-developers\\1.course\\venv\\lib\\site-packages (from spacy) (2.1.3)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.27.1-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading blis-1.0.1-cp312-cp312-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.0.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\desktop\\projects\\generative-ai-for-developers\\1.course\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\admin\\desktop\\projects\\generative-ai-for-developers\\1.course\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\desktop\\projects\\generative-ai-for-developers\\1.course\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.0-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.2-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.8 MB 2.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.3/11.8 MB 2.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.8/11.8 MB 2.2 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.1/11.8 MB 2.0 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.1/11.8 MB 2.0 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.4/11.8 MB 1.7 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.6/11.8 MB 1.6 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.9/11.8 MB 1.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.9/11.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.1/11.8 MB 1.3 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.4/11.8 MB 1.3 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.4/11.8 MB 1.3 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 3.7/11.8 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 3.7/11.8 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.9/11.8 MB 1.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.2/11.8 MB 1.1 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.2/11.8 MB 1.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 4.5/11.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 4.7/11.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 4.7/11.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.0/11.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.2/11.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.2/11.8 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 5.5/11.8 MB 1.0 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.8/11.8 MB 1.0 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.8/11.8 MB 1.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.0/11.8 MB 1.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 6.3/11.8 MB 1.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 6.6/11.8 MB 1.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 6.6/11.8 MB 1.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 6.8/11.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.1/11.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.3/11.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 7.6/11.8 MB 1.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 7.9/11.8 MB 1.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.1/11.8 MB 1.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.4/11.8 MB 1.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.4/11.8 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.7/11.8 MB 1.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 8.9/11.8 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.9/11.8 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.2/11.8 MB 999.0 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.2/11.8 MB 999.0 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.4/11.8 MB 977.0 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.4/11.8 MB 977.0 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.7/11.8 MB 964.7 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.7/11.8 MB 964.7 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 10.0/11.8 MB 950.6 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.0/11.8 MB 950.6 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.2/11.8 MB 945.8 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.5/11.8 MB 944.2 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.5/11.8 MB 944.2 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.7/11.8 MB 932.1 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.7/11.8 MB 932.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.0/11.8 MB 925.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.8 MB 921.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.8 MB 921.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.8 MB 920.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 915.1 kB/s eta 0:00:00\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.10-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.11-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "Downloading pydantic-2.10.2-py3-none-any.whl (456 kB)\n",
      "Downloading pydantic_core-2.27.1-cp312-none-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 840.2 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 840.2 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.8/2.0 MB 819.2 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.8/2.0 MB 819.2 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 799.2 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 799.2 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/2.0 MB 762.6 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 762.6 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.0 MB 682.3 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.0 MB 682.3 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.0 MB 682.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 633.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 623.8 kB/s eta 0:00:00\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp312-cp312-win_amd64.whl (478 kB)\n",
      "Downloading thinc-8.3.2-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 837.5 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.5/1.5 MB 837.5 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 0.8/1.5 MB 745.8 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 729.5 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 729.5 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.3/1.5 MB 737.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 727.3 kB/s eta 0:00:00\n",
      "Using cached numpy-2.0.2-cp312-cp312-win_amd64.whl (15.6 MB)\n",
      "Using cached typer-0.13.1-py3-none-any.whl (44 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.0.1-cp312-cp312-win_amd64.whl (6.4 MB)\n",
      "   ---------------------------------------- 0.0/6.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.4 MB 882.6 kB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.5/6.4 MB 882.6 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.8/6.4 MB 884.1 kB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.0/6.4 MB 883.6 kB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.0/6.4 MB 883.6 kB/s eta 0:00:07\n",
      "   -------- ------------------------------- 1.3/6.4 MB 849.7 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 1.6/6.4 MB 847.3 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 1.6/6.4 MB 847.3 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 1.8/6.4 MB 832.2 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 1.8/6.4 MB 832.2 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 2.1/6.4 MB 804.5 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 2.4/6.4 MB 798.9 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 2.4/6.4 MB 798.9 kB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 2.6/6.4 MB 794.9 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 2.6/6.4 MB 794.9 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 2.9/6.4 MB 776.7 kB/s eta 0:00:05\n",
      "   ------------------- -------------------- 3.1/6.4 MB 769.0 kB/s eta 0:00:05\n",
      "   ------------------- -------------------- 3.1/6.4 MB 769.0 kB/s eta 0:00:05\n",
      "   --------------------- ------------------ 3.4/6.4 MB 771.4 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 3.4/6.4 MB 771.4 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 3.7/6.4 MB 773.5 kB/s eta 0:00:04\n",
      "   ------------------------ --------------- 3.9/6.4 MB 777.9 kB/s eta 0:00:04\n",
      "   -------------------------- ------------- 4.2/6.4 MB 786.4 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 4.2/6.4 MB 786.4 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 4.5/6.4 MB 796.6 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 4.7/6.4 MB 792.3 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 4.7/6.4 MB 792.3 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 5.0/6.4 MB 796.8 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 5.2/6.4 MB 799.0 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 5.2/6.4 MB 799.0 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 5.5/6.4 MB 804.7 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 5.8/6.4 MB 800.7 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.4 MB 800.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.0/6.4 MB 797.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.0/6.4 MB 797.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.4/6.4 MB 793.0 kB/s eta 0:00:00\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.17.0-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, urllib3, typing-extensions, spacy-loggers, spacy-legacy, shellingham, setuptools, numpy, murmurhash, mdurl, MarkupSafe, idna, cloudpathlib, charset-normalizer, certifi, catalogue, annotated-types, srsly, smart-open, requests, pydantic-core, preshed, markdown-it-py, marisa-trie, jinja2, blis, rich, pydantic, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "Successfully installed MarkupSafe-3.0.2 annotated-types-0.7.0 blis-1.0.1 catalogue-2.0.10 certifi-2024.8.30 charset-normalizer-3.4.0 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.10 idna-3.10 jinja2-3.1.4 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.11 numpy-2.0.2 preshed-3.0.9 pydantic-2.10.2 pydantic-core-2.27.1 requests-2.32.3 rich-13.9.4 setuptools-75.6.0 shellingham-1.5.4 smart-open-7.0.5 spacy-3.8.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.3.2 typer-0.13.1 typing-extensions-4.12.2 urllib3-2.2.3 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Admin\\Desktop\\Projects\\Generative-AI-for-Developers\\1.Course\\venv\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Admin\\Desktop\\Projects\\Generative-AI-for-Developers\\1.Course\\venv\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a60bd-ac6e-4847-a022-e1dc5b6087e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install blis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a65b739f-fedd-4baf-a9c4-1d0e1d609e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "#nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a7357-8f0d-4c26-a11c-faff29f03fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc1 = nlp(sent5)\n",
    "# doc2 = nlp(sent6)\n",
    "# doc3 = nlp(sent7)\n",
    "# doc4 = nlp(sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc7b254-69d7-4789-85ff-5dd649df814b",
   "metadata": {},
   "source": [
    "<h1>Stemmer</h1>\n",
    "<p>text preprocessing technique used to reduce words to their root or base form.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2fcc515e-a6a8-4e5c-bd14-c1cd2bef8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f862fef-57a2-4ca1-9841-7ca7a127f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "106fbd76-f516-4eed-8179-dfd73ca63449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a00ffa9c-a84a-43e9-a066-60b65a4bc767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get old despit my have seen it some 15 or more time in the last 25 year paul luka perform bring tear to my eye and bett davi in one of her veri few truli sympathet role is a delight the kid are as grandma say more like dressedup midget than children but that onli make them more fun to watch and the mother slow awaken to what happen in the world and under her own roof is believ and startl if i had a dozen thumb theyd all be up for thi movi'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie'\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d350605-2916-456d-86bc-0d4a0ecba488",
   "metadata": {},
   "source": [
    "<h1>Lemmatization</h1>\n",
    "<p>Lemmatization is the process of grouping together different inflected forms of the same word. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "12f59916-36de-43ae-b246-29213cd95db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "987d5617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405b428",
   "metadata": {},
   "source": [
    "NOTE: Stemming & lamatization are same to retrieve root words but lamatization is worked good. Lamatization is slow & stemming is fast"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
